{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dke3CXxJAivz"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from functools import partial\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFtSaJzAngL"
      },
      "source": [
        "!pip install efficientnet\n",
        "import efficientnet.tfkeras as efn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3CXtT-aAqc6"
      },
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.compat.v1.set_random_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrIFjdHsBilV"
      },
      "source": [
        "from skimage.filters import rank, threshold_otsu\n",
        "from skimage.color import rgb2grey\n",
        "from sklearn.cluster import KMeans\n",
        "from skimage.morphology import closing, square, disk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTb7pJzcBkn6"
      },
      "source": [
        "def plot_any(arr, title = ''):\n",
        "    \"\"\"\n",
        "    plot multiple pictures\n",
        "    \"\"\"\n",
        "    plt.figure(figsize = (15, 25))\n",
        "    for i in range(len(arr)):\n",
        "        plt.subplot(1,len(arr),i + 1)\n",
        "        plt.title(title)\n",
        "        plt.imshow(arr[i]);\n",
        "\n",
        "        \n",
        "def d2Kmeans(img, k):\n",
        "    \"\"\"\n",
        "    Apply 2 dimensional KMeans algorithm on pictures\n",
        "    \"\"\"\n",
        "    return KMeans(n_jobs=-1, \n",
        "                  random_state=1, \n",
        "                  n_clusters = k, \n",
        "                  init='k-means++'\n",
        "    ).fit(img.reshape((-1,1))).labels_.reshape(img.shape)\n",
        "\n",
        "\n",
        "def merge_segmented_mask_ROI(uri_img, img_kluster):\n",
        "    \"\"\"\n",
        "    Merge original pricture and segmented picture\n",
        "    \"\"\"\n",
        "    new_img = uri_img.copy()\n",
        "    for ch in range(3):\n",
        "        new_img[:,:, ch] *= img_kluster\n",
        "    return new_img\n",
        "\n",
        "\n",
        "def mean_filter(image, radius):\n",
        "    \"\"\"\n",
        "    Create smooth boundaries of segmenation thourgh applying a gaussian mean blur\n",
        "    \"\"\"\n",
        "    return rank.mean_percentile(image, selem = disk(radius))\n",
        "\n",
        "\n",
        "def binary(image):\n",
        "    \"\"\"\n",
        "    Get round boundaries of the image when segmenting\n",
        "    \"\"\"\n",
        "    return image > threshold_otsu(image)\n",
        "\n",
        "\n",
        "def select_cluster_index(clusters):\n",
        "    \"\"\"\n",
        "    Chooose the right cluster index, which is the smallest, as the rest is background\n",
        "    \"\"\"\n",
        "    minx = clusters[0].mean()\n",
        "    index = 0\n",
        "    for i in clusters:\n",
        "        if i.mean() < minx:\n",
        "            minx = i.mean()\n",
        "            index += 1\n",
        "    return index\n",
        "\n",
        "\n",
        "def segment_image(img, k = 2):\n",
        "    \"\"\"\n",
        "    segment the image in skin mole versus background\n",
        "    \"\"\"\n",
        "    # Cluster the image\n",
        "    result_gray = d2Kmeans(rgb2grey(img), k)\n",
        "    \n",
        "    # Select the correct cluster\n",
        "    clusters_gray = [result_gray == i for i in range(k)]\n",
        "    selected_index = select_cluster_index(clusters_gray)\n",
        "    results_gray = clusters_gray[selected_index]\n",
        "    \n",
        "    # Apply smoothing of the boundaries\n",
        "    image_mean_filter = mean_filter(results_gray, 20)\n",
        "    test_binary = binary(image_mean_filter)\n",
        "    \n",
        "    # Create segmented picture with black background\n",
        "    new_img = merge_segmented_mask_ROI(img, test_binary)\n",
        "    \n",
        "    return new_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjkZzTBwB4lY"
      },
      "source": [
        "#Transfer 'jpg' images to an array IMG\n",
        "def Dataset_loader(DIR, RESIZE):\n",
        "    IMG = []\n",
        "    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
        "    for IMAGE_NAME in tqdm(os.listdir(DIR)):\n",
        "        PATH = os.path.join(DIR,IMAGE_NAME)\n",
        "        _, ftype = os.path.splitext(PATH)\n",
        "        if ftype == \".jpg\":\n",
        "            img = read(PATH)\n",
        "            img = cv2.resize(img, (RESIZE,RESIZE))\n",
        "            img = segment_image(img)\n",
        "            IMG.append(np.array(img))\n",
        "    return IMG\n",
        "\n",
        "benign_train = np.array(Dataset_loader('/content/drive/MyDrive/dataset/data/train/benign',224))\n",
        "malign_train = np.array(Dataset_loader('/content/drive/MyDrive/dataset/data/train/malignant',224))\n",
        "benign_test = np.array(Dataset_loader('/content/drive/MyDrive/dataset/data/test/benign',224))\n",
        "malign_test = np.array(Dataset_loader('/content/drive/MyDrive/dataset/data/test/malignant',224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGHJPUsqB8fF"
      },
      "source": [
        "plot_any(benign_train[:5], title = \"Benign Train\")\n",
        "plot_any(malign_train[:5], title = \"Malignant Train\")\n",
        "plot_any(benign_test[:5], title = \"Benign Test\")\n",
        "plot_any(malign_test[:5], title = \"Malignant Test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WkUcwTQL3Pq"
      },
      "source": [
        "# Skin Cancer: Malignant vs. Benign\n",
        "# Create labels\n",
        "benign_train_label = np.zeros(len(benign_train))\n",
        "malign_train_label = np.ones(len(malign_train))\n",
        "benign_test_label = np.zeros(len(benign_test))\n",
        "malign_test_label = np.ones(len(malign_test))\n",
        "\n",
        "# Merge data \n",
        "X_train = np.concatenate((benign_train, malign_train), axis = 0)\n",
        "Y_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)\n",
        "X_test = np.concatenate((benign_test, malign_test), axis = 0)\n",
        "Y_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)\n",
        "\n",
        "# Shuffle train data\n",
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train = X_train[s]\n",
        "Y_train = Y_train[s]\n",
        "\n",
        "# Shuffle test data\n",
        "s = np.arange(X_test.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_test = X_test[s]\n",
        "Y_test = Y_test[s]\n",
        "\n",
        "# To categorical\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, num_classes= 2)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, num_classes= 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKgdpbmsL_Ml"
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    X_train, Y_train, \n",
        "    test_size=0.2, \n",
        "    random_state=SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGNuUPZkMFXn"
      },
      "source": [
        "# # Display first 15 images of moles, and how they are classified\n",
        "w=60\n",
        "h=40\n",
        "fig=plt.figure(figsize=(15, 15))\n",
        "columns = 4\n",
        "rows = 3\n",
        "\n",
        "for i in range(1, columns*rows +1):\n",
        "    ax = fig.add_subplot(rows, columns, i)\n",
        "    if np.argmax(Y_train[i]) == 0:\n",
        "        ax.title.set_text('Benign')\n",
        "    else:\n",
        "        ax.title.set_text('Malignant')\n",
        "    plt.imshow(x_train[i], interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oniRp488MJ0o"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Add Image augmentation to our generator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=360,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   zoom_range=(0.75,1),\n",
        "                                   brightness_range=(0.75,1.25)\n",
        "                                  )\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
        "val_generator = train_datagen.flow(x_val, y_val, batch_size=BATCH_SIZE, shuffle= False)\n",
        "test_generator = test_datagen.flow(X_test, Y_test, batch_size=BATCH_SIZE, shuffle= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHsW5a8AMPSD"
      },
      "source": [
        "from skimage import io\n",
        "\n",
        "def imshow(image_RGB):\n",
        "    io.imshow(image_RGB)\n",
        "    io.show()\n",
        "\n",
        "x1, y1 = train_generator[0]\n",
        "x2, y2 = val_generator[0]\n",
        "x3, y3 = test_generator[0]\n",
        "\n",
        "imshow(x1[0])\n",
        "imshow(x2[0])\n",
        "imshow(x3[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpXGB8kKMSOv"
      },
      "source": [
        "def build_model(backbone, lr=1e-4):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(backbone)\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op1OKwVZNm8E"
      },
      "source": [
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAAvrcOIMYQu"
      },
      "source": [
        "from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\n",
        "from tensorflow.python import ops\n",
        "from tensorflow.python.ops import math_ops, state_ops, control_flow_ops\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "__all__ = ['RAdam']\n",
        "\n",
        "\n",
        "class RAdam(OptimizerV2):\n",
        "    \"\"\"RAdam optimizer.\n",
        "    According to the paper\n",
        "    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 learning_rate=0.001,\n",
        "                 beta_1=0.9,\n",
        "                 beta_2=0.999,\n",
        "                 epsilon=1e-7,\n",
        "                 weight_decay=0.,\n",
        "                 amsgrad=False,\n",
        "                 total_steps=0,\n",
        "                 warmup_proportion=0.1,\n",
        "                 min_lr=0.,\n",
        "                 name='RAdam',\n",
        "                 **kwargs):\n",
        "        r\"\"\"Construct a new Adam optimizer.\n",
        "        Args:\n",
        "            learning_rate: A Tensor or a floating point value.    The learning rate.\n",
        "            beta_1: A float value or a constant float tensor. The exponential decay\n",
        "                rate for the 1st moment estimates.\n",
        "            beta_2: A float value or a constant float tensor. The exponential decay\n",
        "                rate for the 2nd moment estimates.\n",
        "            epsilon: A small constant for numerical stability. This epsilon is\n",
        "                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
        "                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
        "            weight_decay: A floating point value. Weight decay for each param.\n",
        "            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n",
        "                the paper \"On the Convergence of Adam and beyond\".\n",
        "            total_steps: An integer. Total number of training steps.\n",
        "                Enable warmup by setting a positive value.\n",
        "            warmup_proportion: A floating point value. The proportion of increasing steps.\n",
        "            min_lr: A floating point value. Minimum learning rate after warmup.\n",
        "            name: Optional name for the operations created when applying gradients.\n",
        "                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n",
        "                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n",
        "                a callable that takes no arguments and returns the actual value to use.\n",
        "                This can be useful for changing these values across different\n",
        "                invocations of optimizer functions. @end_compatibility\n",
        "            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
        "                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
        "                gradients by value, `decay` is included for backward compatibility to\n",
        "                allow time inverse decay of learning rate. `lr` is included for backward\n",
        "                compatibility, recommended to use `learning_rate` instead.\n",
        "        \"\"\"\n",
        "\n",
        "        super(RAdam, self).__init__(name, **kwargs)\n",
        "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
        "        self._set_hyper('beta_1', beta_1)\n",
        "        self._set_hyper('beta_2', beta_2)\n",
        "        self._set_hyper('decay', self._initial_decay)\n",
        "        self._set_hyper('weight_decay', weight_decay)\n",
        "        self._set_hyper('total_steps', float(total_steps))\n",
        "        self._set_hyper('warmup_proportion', warmup_proportion)\n",
        "        self._set_hyper('min_lr', min_lr)\n",
        "        self.epsilon = epsilon or K.epsilon()\n",
        "        self.amsgrad = amsgrad\n",
        "        self._initial_weight_decay = weight_decay\n",
        "        self._initial_total_steps = total_steps\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, 'm')\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, 'v')\n",
        "        if self.amsgrad:\n",
        "            for var in var_list:\n",
        "                self.add_slot(var, 'vhat')\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        params = self.weights\n",
        "        num_vars = int((len(params) - 1) / 2)\n",
        "        if len(weights) == 3 * num_vars + 1:\n",
        "            weights = weights[:len(params)]\n",
        "        super(RAdam, self).set_weights(weights)\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        lr_t = self._decayed_lr(var_dtype)\n",
        "        m = self.get_slot(var, 'm')\n",
        "        v = self.get_slot(var, 'v')\n",
        "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
        "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
        "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
        "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
        "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
        "\n",
        "        if self._initial_total_steps > 0:\n",
        "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
        "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
        "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
        "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
        "            decay_rate = (min_lr - lr_t) / decay_steps\n",
        "            lr_t = tf.where(\n",
        "                local_step <= warmup_steps,\n",
        "                lr_t * (local_step / warmup_steps),\n",
        "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
        "            )\n",
        "\n",
        "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
        "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
        "\n",
        "        m_t = state_ops.assign(m,\n",
        "                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n",
        "                               use_locking=self._use_locking)\n",
        "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
        "\n",
        "        v_t = state_ops.assign(v,\n",
        "                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n",
        "                               use_locking=self._use_locking)\n",
        "        if self.amsgrad:\n",
        "            vhat = self.get_slot(var, 'vhat')\n",
        "            vhat_t = state_ops.assign(vhat,\n",
        "                                      math_ops.maximum(vhat, v_t),\n",
        "                                      use_locking=self._use_locking)\n",
        "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
        "        else:\n",
        "            vhat_t = None\n",
        "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
        "\n",
        "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
        "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
        "                            sma_inf / sma_t)\n",
        "\n",
        "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
        "\n",
        "        if self._initial_weight_decay > 0.0:\n",
        "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
        "\n",
        "        var_update = state_ops.assign_sub(var,\n",
        "                                          lr_t * var_t,\n",
        "                                          use_locking=self._use_locking)\n",
        "\n",
        "        updates = [var_update, m_t, v_t]\n",
        "        if self.amsgrad:\n",
        "            updates.append(vhat_t)\n",
        "        return control_flow_ops.group(*updates)\n",
        "\n",
        "    def _resource_apply_sparse(self, grad, var, indices):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        lr_t = self._decayed_lr(var_dtype)\n",
        "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
        "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
        "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
        "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
        "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
        "\n",
        "        if self._initial_total_steps > 0:\n",
        "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
        "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
        "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
        "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
        "            decay_rate = (min_lr - lr_t) / decay_steps\n",
        "            lr_t = tf.where(\n",
        "                local_step <= warmup_steps,\n",
        "                lr_t * (local_step / warmup_steps),\n",
        "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
        "            )\n",
        "\n",
        "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
        "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
        "\n",
        "        m = self.get_slot(var, 'm')\n",
        "        m_scaled_g_values = grad * (1 - beta_1_t)\n",
        "        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n",
        "        with ops.control_dependencies([m_t]):\n",
        "            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
        "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
        "\n",
        "        v = self.get_slot(var, 'v')\n",
        "        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n",
        "        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n",
        "        with ops.control_dependencies([v_t]):\n",
        "            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
        "\n",
        "        if self.amsgrad:\n",
        "            vhat = self.get_slot(var, 'vhat')\n",
        "            vhat_t = state_ops.assign(vhat,\n",
        "                                      math_ops.maximum(vhat, v_t),\n",
        "                                      use_locking=self._use_locking)\n",
        "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
        "        else:\n",
        "            vhat_t = None\n",
        "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
        "\n",
        "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
        "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
        "                            sma_inf / sma_t)\n",
        "\n",
        "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
        "\n",
        "        if self._initial_weight_decay > 0.0:\n",
        "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
        "\n",
        "        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n",
        "\n",
        "        updates = [var_update, m_t, v_t]\n",
        "        if self.amsgrad:\n",
        "            updates.append(vhat_t)\n",
        "        return control_flow_ops.group(*updates)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(RAdam, self).get_config()\n",
        "        config.update({\n",
        "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
        "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
        "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
        "            'decay': self._serialize_hyperparameter('decay'),\n",
        "            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n",
        "            'epsilon': self.epsilon,\n",
        "            'amsgrad': self.amsgrad,\n",
        "            'total_steps': self._serialize_hyperparameter('total_steps'),\n",
        "            'warmup_proportion': self._serialize_hyperparameter('warmup_proportion'),\n",
        "            'min_lr': self._serialize_hyperparameter('min_lr'),\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ0XrMRZMc3r"
      },
      "source": [
        "efficientnetb3 = efn.EfficientNetB0(\n",
        "        weights='imagenet',\n",
        "        input_shape=(224,224,3),\n",
        "        include_top=False\n",
        "                   )\n",
        "\n",
        "model = build_model(efficientnetb3)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rYmxexTMhGk"
      },
      "source": [
        "model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer = RAdam(learning_rate=1e-3, \n",
        "                          min_lr=1e-7,\n",
        "                          warmup_proportion=0.15),\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pKOx7SHkebJ"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmOnzBvxMhoG"
      },
      "source": [
        "# Learning Rate Reducer\n",
        "learn_control = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                                  patience=5,\n",
        "                                  verbose=1,\n",
        "                                  factor=0.2, \n",
        "                                  min_lr=1e-7)\n",
        "\n",
        "# Checkpoint\n",
        "filepath=\"/content/drive/MyDrive/EfficientNetB0/weights.best.hdf5\"\n",
        "checkpoint_callbacks = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CxqHhaZPIHy"
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=x_train.shape[0] // BATCH_SIZE,\n",
        "    epochs=30,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps = x_val.shape[0] // BATCH_SIZE,\n",
        "    callbacks = [learn_control, checkpoint]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zflrQ6cE9Sv6"
      },
      "source": [
        "with open('history.json', 'w') as f:\n",
        "    json.dump(str(history.history), f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY6TmWdw9U78"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['accuracy', 'val_accuracy']].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8BPpmvG9Xx0"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['loss', 'val_loss']].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emrcoxh59aOU"
      },
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/EfficientNetB0/weights.best.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GPZk-kI9dGK"
      },
      "source": [
        "val_generator.reset()\n",
        "Y_val_pred = model.predict_generator(val_generator, steps=np.ceil(x_val.shape[0]/BATCH_SIZE))\n",
        "accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBy78RsV9fI0"
      },
      "source": [
        "test_generator.reset()\n",
        "Y_pred = model.predict_generator(test_generator, steps=np.ceil(X_test.shape[0]/BATCH_SIZE))\n",
        "accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MW8UCI49hg-"
      },
      "source": [
        "tta_steps = 10\n",
        "predictions = []\n",
        "\n",
        "for i in tqdm(range(tta_steps)):\n",
        "    test_generator = train_datagen.flow(X_test, Y_test, batch_size=BATCH_SIZE, shuffle= False)\n",
        "    preds = model.predict_generator(test_generator, steps=np.ceil(X_test.shape[0]/BATCH_SIZE))\n",
        "    predictions.append(preds)\n",
        "\n",
        "    del test_generator\n",
        "    gc.collect()\n",
        "    \n",
        "Y_pred_tta = np.mean(predictions, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlmQ29De9kgz"
      },
      "source": [
        "accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "529P_4Hd9mUg"
      },
      "source": [
        "accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzp-nrHB9m0C"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=55)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "cm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))\n",
        "\n",
        "cm_plot_label =['benign', 'malignant']\n",
        "plot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for Skin Cancer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu1_DeQJ9pPK"
      },
      "source": [
        "cm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\n",
        "\n",
        "cm_plot_label =['benign', 'malignant']\n",
        "plot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for Skin Cancer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ61hWgV9rrp"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "classification_report( np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBj1X7u9uH9"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, auc\n",
        "from sklearn.metrics import roc_curve\n",
        "roc_log = roc_auc_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\n",
        "false_positive_rate, true_positive_rate, threshold = roc_curve(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\n",
        "area_under_curve = auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.plot(false_positive_rate, true_positive_rate, label='AUC = {:.3f}'.format(area_under_curve))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "#plt.savefig(ROC_PLOT_FILE, bbox_inches='tight')\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoDhVtvb9wgK"
      },
      "source": [
        "i=0\n",
        "prop_class=[]\n",
        "mis_class=[]\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "    if(np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n",
        "        prop_class.append(i)\n",
        "    if(len(prop_class)==8):\n",
        "        break\n",
        "\n",
        "i=0\n",
        "for i in range(len(Y_test)):\n",
        "    if(not np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n",
        "        mis_class.append(i)\n",
        "    if(len(mis_class)==8):\n",
        "        break\n",
        "\n",
        "# # Display first 8 images of benign\n",
        "w=60\n",
        "h=40\n",
        "fig=plt.figure(figsize=(18, 10))\n",
        "columns = 4\n",
        "rows = 2\n",
        "\n",
        "def Transfername(namecode):\n",
        "    if namecode==0:\n",
        "        return \"Benign\"\n",
        "    else:\n",
        "        return \"Malignant\"\n",
        "    \n",
        "for i in range(len(prop_class)):\n",
        "    ax = fig.add_subplot(rows, columns, i+1)\n",
        "    ax.set_title(\"Predicted result:\"+ Transfername(np.argmax(Y_pred_tta[prop_class[i]]))\n",
        "                       +\"\\n\"+\"Actual result: \"+ Transfername(np.argmax(Y_test[prop_class[i]])))\n",
        "    plt.imshow(X_test[prop_class[i]], interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_HvMPcU9y-9"
      },
      "source": [
        "# save model\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "\n",
        "with open(\"efficientnetb0.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"efficientnet.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCkwwYKdf3bV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN4LD2pUf8cL"
      },
      "source": [
        "i=0\n",
        "prop_class=[]\n",
        "mis_class=[]\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "    if(np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n",
        "        prop_class.append(i)\n",
        "    if(len(prop_class)==8):\n",
        "        break\n",
        "\n",
        "i=0\n",
        "for i in range(len(Y_test)):\n",
        "    if(not np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n",
        "        mis_class.append(i)\n",
        "    if(len(mis_class)==8):\n",
        "        break\n",
        "\n",
        "# # Display first 8 images of benign\n",
        "w=60\n",
        "h=40\n",
        "fig=plt.figure(figsize=(18, 10))\n",
        "columns = 4\n",
        "rows = 2\n",
        "\n",
        "def Transfername(namecode):\n",
        "    if namecode==0:\n",
        "        return \"Benign\"\n",
        "    else:\n",
        "        return \"Malignant\"\n",
        "    \n",
        "for i in range(len(mis_class)):\n",
        "    ax = fig.add_subplot(rows, columns, i+1)\n",
        "    ax.set_title(\"Predicted result:\"+ Transfername(np.argmax(Y_pred_tta[mis_class[i]]))\n",
        "                       +\"\\n\"+\"Actual result: \"+ Transfername(np.argmax(Y_test[mis_class[i]])))\n",
        "    plt.imshow(X_test[mis_class[i]], interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}